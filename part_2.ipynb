{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"part_2.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_sU5o1SIMkDE","colab_type":"text"},"source":["# Import modules"]},{"cell_type":"code","metadata":{"id":"HSdJl_jsMkDG","colab_type":"code","colab":{},"outputId":"8522928e-7a4a-44ba-b70b-b27f9a14f57e"},"source":["import pandas as pd\n","import numpy as np\n","import requests\n","\n","import sklearn\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n","from sklearn.utils import shuffle\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n","from sklearn.feature_selection import chi2, SelectKBest\n","\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('vader_lexicon')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package vader_lexicon to\n","[nltk_data]     /home/c1977808/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"Bojhh13AMkDM","colab_type":"text"},"source":["# Read data"]},{"cell_type":"code","metadata":{"id":"AnHDYfkOMkDN","colab_type":"code","colab":{}},"source":["def read_data(path_pos, path_neg):\n","    pos = pd.read_csv(path_pos, sep=\"\\n\", header=None, names=['review'])\n","    pos['positive']=1\n","    neg = pd.read_csv(path_neg, sep=\"\\n\", header=None, names=['review'])\n","    neg['positive']=0\n","    combined_df = pos.append(neg)\n","    combined_df = shuffle(combined_df, random_state=42)\n","    return(combined_df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_C8peiUhMkDR","colab_type":"code","colab":{}},"source":["# read in training data\n","train = read_data(path_pos=\"Data/IMDb/train/imdb_train_pos.txt\",\n","                  path_neg=\"Data/IMDb/train/imdb_train_neg.txt\")\n","\n","dev = read_data(path_pos=\"Data/IMDb/dev/imdb_dev_pos.txt\",\n","                path_neg=\"Data/IMDb/dev/imdb_dev_neg.txt\")\n","\n","test = read_data(path_pos=\"Data/IMDb/test/imdb_test_pos.txt\",\n","                 path_neg=\"Data/IMDb/test/imdb_test_neg.txt\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6sZe6IsMkDT","colab_type":"text"},"source":["# Explore data and shuffle"]},{"cell_type":"code","metadata":{"id":"vMzyuuQUMkDU","colab_type":"code","colab":{},"outputId":"7edb45cf-2c97-4622-eb87-5ada1c16fca9"},"source":["train.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>positive</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4016</th>\n","      <td>This is a comedy/romance movie directed by And...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6475</th>\n","      <td>During the Sci-Fi TZ marathon of January 31, 1...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5684</th>\n","      <td>radio is possibly one of the best films i have...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>862</th>\n","      <td>I was -Unlike most of the reviewers- not born ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5970</th>\n","      <td>When i started watching \"Surface\"for the first...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                 review  positive\n","4016  This is a comedy/romance movie directed by And...         0\n","6475  During the Sci-Fi TZ marathon of January 31, 1...         1\n","5684  radio is possibly one of the best films i have...         0\n","862   I was -Unlike most of the reviewers- not born ...         1\n","5970  When i started watching \"Surface\"for the first...         1"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"cVaLSeGDMkDX","colab_type":"code","colab":{},"outputId":"51183306-f9dc-4af5-fe9e-b21ed6e48a12"},"source":["print(\"No of positive reviews\\n-----\")\n","print(train['positive'].value_counts()[1])\n","print(\"\\nNo of negative reviews\\n-----\")\n","print(train['positive'].value_counts()[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["No of positive reviews\n","-----\n","7483\n","\n","No of negative reviews\n","-----\n","7517\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kI1Jez1NMkDZ","colab_type":"text"},"source":["# Feature 1 - tf-idf"]},{"cell_type":"markdown","metadata":{"id":"phRr2SrBMkDa","colab_type":"text"},"source":["The first feature is based on a vocabulary of tokens collected from the training data. Term Frequency times Inverse Document Frequency will be used. Term Frequency (tf) counts the number of times a token in the vocabulary is used in each review, relative to how frequently it appears in that review. This acts as a way of normalising the count. Inverse Document Frequency (idf) penalises tokens that appear across many reviews, as these terms offer less information than those that appear in fewer reviews. "]},{"cell_type":"markdown","metadata":{"id":"q4_iXraCMkDb","colab_type":"text"},"source":["## Create set of stopwords to remove later"]},{"cell_type":"code","metadata":{"id":"PFCkjI3sMkDb","colab_type":"code","colab":{}},"source":["# take set of stopwords from nltk\n","stopwords=set(nltk.corpus.stopwords.words('english'))\n","# manually add more punctuation\n","stopwords.add(\".\")\n","stopwords.add(\",\")\n","stopwords.add(\"--\")\n","stopwords.add(\"``\")\n","stopwords.add(\"#\")\n","stopwords.add(\"@\")\n","stopwords.add(\":\")\n","stopwords.add(\"'s\")\n","stopwords.add(\"â€™\")\n","stopwords.add(\"...\")\n","stopwords.add(\"n't\")\n","stopwords.add(\"'re\")\n","stopwords.add(\"'\")\n","stopwords.add(\"-\")\n","stopwords.add(\";\")\n","stopwords.add(\"/\")\n","stopwords.add(\">\")\n","stopwords.add(\"<\")\n","stopwords.add(\"br\")\n","stopwords.add(\"(\")\n","stopwords.add(\")\")\n","stopwords.add(\"''\")\n","stopwords.add(\"&\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AO0EkwdIMkDd","colab_type":"text"},"source":["## Define custom transformers"]},{"cell_type":"markdown","metadata":{"id":"DaWwjrAAMkDe","colab_type":"text"},"source":["Need to create a simple transormer so just the dataset can be used to feed into the full pipeline later. Currently, feature 1 pipeline takes `train['review']` while feature 2 pipeline takes `train` as it's argument. This needs to be consistent."]},{"cell_type":"code","metadata":{"id":"xUOrmrZuMkDf","colab_type":"code","colab":{}},"source":["class selectReview(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        return None\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        return(X['review'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiTEwrWWMkDi","colab_type":"text"},"source":["## Create transformation pipeline"]},{"cell_type":"code","metadata":{"id":"J3SgHcQrMkDj","colab_type":"code","colab":{}},"source":["feature_1_vocab = Pipeline([\n","    ('select_review', selectReview()),\n","    ('count', CountVectorizer(stop_words=stopwords, max_features=500)),\n","    ('tfidf', TfidfTransformer())\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79EO-deaMkDl","colab_type":"text"},"source":["# Feature 2 - tf-idf (bi-grams)"]},{"cell_type":"code","metadata":{"id":"_YlA599HMkDm","colab_type":"code","colab":{}},"source":["feature_2_vocab = Pipeline([\n","    ('select_review', selectReview()),\n","    ('count', CountVectorizer(stop_words=stopwords, max_features=500, ngram_range=(2,2))),\n","    ('tfidf', TfidfTransformer())\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RtijCZ-MkDo","colab_type":"text"},"source":["# Feature 3 - Sentiment"]},{"cell_type":"code","metadata":{"id":"OJGlBewBMkDp","colab_type":"code","colab":{}},"source":["vader = SentimentIntensityAnalyzer()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1NyOFu-WMkDr","colab_type":"text"},"source":["## Define custom transformers"]},{"cell_type":"code","metadata":{"id":"2cfM4CdKMkDr","colab_type":"code","colab":{}},"source":["class getSentiment(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        return None\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        features_array=[]\n","        for index, row in X.iterrows():\n","            pos = vader.polarity_scores(row['review'])['pos']\n","            neu = vader.polarity_scores(row['review'])['neu']\n","            neg = vader.polarity_scores(row['review'])['neg']\n","            features_array.append([pos, neu, neg])\n","        return(np.asarray(features_array))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4f-eZvvMkDt","colab_type":"text"},"source":["## Create transformation pipeline"]},{"cell_type":"code","metadata":{"id":"bbYwiRUhMkDu","colab_type":"code","colab":{}},"source":["feature_3_sentiment = Pipeline([\n","    ('get_sentiment', getSentiment())\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cRBb3ZBqMkDw","colab_type":"text"},"source":["# Combine all features"]},{"cell_type":"code","metadata":{"id":"ipQRzmYSMkDx","colab_type":"code","colab":{}},"source":["feature_engineering = FeatureUnion(transformer_list=[\n","    (\"feature_1_vocab\", feature_1_vocab),\n","    (\"feature_2_vocab\", feature_2_vocab),\n","    (\"feature_3_sentiment\", feature_3_sentiment)\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x8TYOANKMkDz","colab_type":"code","colab":{}},"source":["X_train = feature_engineering.fit_transform(train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ga1Ye0G0MkD1","colab_type":"code","colab":{},"outputId":"9ec3b967-8b4c-46c3-a4cc-c3be9643640d"},"source":["print(X_train.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(15000, 1003)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Dju_tNFMkD3","colab_type":"text"},"source":["# Perform feature selection"]},{"cell_type":"markdown","metadata":{"id":"xzCsfFSdMkD4","colab_type":"text"},"source":["*Currently unsure whether to perform feature selection as part of Pipeline or through a standard function. Pipeline offers auto tuning of parameters but function allows flexibility to specify test, dev, or train set to be used!*"]},{"cell_type":"code","metadata":{"id":"3bndMMgpMkD5","colab_type":"code","colab":{}},"source":["def feature_selection(matrix, data_split, k):\n","    reduced_features = SelectKBest(chi2, k=k).fit_transform(matrix, np.asarray(data_split['positive']))\n","    return(reduced_features)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6-wDDTnMkD-","colab_type":"code","colab":{}},"source":["X_train_reduced = feature_selection(X_train, train, 10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFKgAKbXMkEA","colab_type":"text"},"source":["# Run and evaluate different models"]},{"cell_type":"markdown","metadata":{"id":"NpfrJlBGMkEA","colab_type":"text"},"source":["Try a series of different machine learning algorithms here such as; SVM (Linear, Polynomial and RBF), SGD, Decision Trees, Logistic Regression. Using GridSearch to test for the best hyperparameters."]},{"cell_type":"code","metadata":{"id":"qPuRwttPMkEB","colab_type":"code","colab":{}},"source":["y_train = np.asarray(train['positive'])\n","y_dev = np.asarray(dev['positive'])\n","X_dev = feature_engineering.transform(dev)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UreNKU9oMkED","colab_type":"text"},"source":["## SVM"]},{"cell_type":"markdown","metadata":{"id":"jaR8X-QaMkEE","colab_type":"text"},"source":["### Linear"]},{"cell_type":"code","metadata":{"id":"IzPj4uNBMkEF","colab_type":"code","colab":{}},"source":["svm_clf = Pipeline([\n","    (\"scaler\", StandardScaler(with_mean=False)),\n","    (\"svm_clf\", sklearn.svm.LinearSVC(loss='hinge'))\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rt8-f6HzMkEH","colab_type":"code","colab":{},"outputId":"1ec98804-1383-4c80-f06a-ec2e8a65d61a"},"source":["svm_clf.fit(X_train, y_train)\n","svm_clf_pred = svm_clf.predict(X_dev)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/home/c1977808/.local/share/virtualenvs/c1977808-tkY5RK3O/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"L42ifILrMkEJ","colab_type":"code","colab":{},"outputId":"cf332ea6-71a4-44a6-ec26-7f0c0f55857f"},"source":["print(classification_report(y_dev, svm_clf_pred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.83      0.85      0.84      2482\n","           1       0.85      0.82      0.84      2518\n","\n","    accuracy                           0.84      5000\n","   macro avg       0.84      0.84      0.84      5000\n","weighted avg       0.84      0.84      0.84      5000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dh7YeFx0MkEL","colab_type":"text"},"source":["### RBF"]},{"cell_type":"code","metadata":{"id":"31YxVmPEMkEL","colab_type":"code","colab":{}},"source":["rbf_svm_clf = Pipeline([\n","    (\"scaler\", StandardScaler(with_mean=False)),\n","    (\"svm_clf\", sklearn.svm.SVC(kernel=\"rbf\"))\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mn_vaJBSMkEO","colab_type":"code","colab":{}},"source":["rbf_svm_clf.fit(X_train, y_train)\n","rbf_svm_clf_pred = rbf_svm_clf.predict(X_dev)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8DUMZgLzMkEQ","colab_type":"code","colab":{},"outputId":"3be2e848-e8fa-4452-e3d7-730dec2b9b6f"},"source":["print(classification_report(y_dev, rbf_svm_clf_pred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.85      0.85      2482\n","           1       0.85      0.85      0.85      2518\n","\n","    accuracy                           0.85      5000\n","   macro avg       0.85      0.85      0.85      5000\n","weighted avg       0.85      0.85      0.85      5000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1_dylF-iMkET","colab_type":"text"},"source":["### Polynomial"]},{"cell_type":"code","metadata":{"id":"_1ylhAVgMkET","colab_type":"code","colab":{}},"source":["poly_svm_clf = Pipeline([\n","    (\"scaler\", StandardScaler(with_mean=False)),\n","    (\"svm_clf\", sklearn.svm.SVC(kernel=\"poly\", degree=3, coef0=1))\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gz9E8be9MkEW","colab_type":"code","colab":{}},"source":["poly_svm_clf.fit(X_train, y_train)\n","poly_svm_clf_pred = poly_svm_clf.predict(X_dev)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7VskYMI2MkEY","colab_type":"code","colab":{},"outputId":"2230e52a-08e9-4a4c-e1ba-c3da67d7c8a7"},"source":["print(classification_report(y_dev, poly_svm_clf_pred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.85      0.85      0.85      2482\n","           1       0.85      0.85      0.85      2518\n","\n","    accuracy                           0.85      5000\n","   macro avg       0.85      0.85      0.85      5000\n","weighted avg       0.85      0.85      0.85      5000\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7wWh0Pj4MkEa","colab_type":"text"},"source":["**NEXT STEPS: run polynomial and RBF SVM using the kernel trick. Then run a decision tree and logistic regression to shortlist ~ five suitable models. Then run `GridSearchCV` to find the best combination of hyperparameters for each of the ~ five models. Run this inside a loop that performs feature selection to reduced the number of variables to 10, 100, 500, 1000?. Finally, run the best performing model on the unseen test data.**"]}]}