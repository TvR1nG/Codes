{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"part_1.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"JtRhiKEavlPl","colab_type":"text"},"source":["# Theory (15%)"]},{"cell_type":"markdown","metadata":{"id":"0B-OFiJcvlPn","colab_type":"text"},"source":["## Question 1\n","\n","*What is the difference between a rule-based system and a machine learning\n","system? (5%)*\n","\n","A rule-based system involves a set of pre-defined rules that are applied to data. The output of which are known as the answers. This system does not seek to learn from the input provided, it simply executes the rules defined. For example, if the height of a person is above 180cm label them 'tall', otherwise label them 'small'. \n","\n","A machine learning system is trained, rather than defined, by providing it with data and answers (only if it is supervised machine learning). The system looks for statistical structure in this input and outputs the rules to be used. For example, predict the height of school children in Cardiff by learning from data that provides the age and height of school children in Swansea."]},{"cell_type":"markdown","metadata":{"id":"dfU1PXA0vlPo","colab_type":"text"},"source":["## Question 2\n","\n","*What is the difference between unsupervised and supervised learning? (5%)*\n","\n","Supervised learning is when a machine learning system is provided the output variable as well as the input variables. By providing the system with the 'answer' it is able learn the statistical characteristics of an observation that leads to a particular output (continuous or categorical). The resulting algorithm can then be used to predict future, unknown outputs. For example, predicting height given age by providing the height and age of school children.\n","\n","Unsupervised learning is when a machine learning system is only provided with input variables, no output variables are supplied. The system can't predict outcome as no 'answer' is supplied. Instead, it has to produce an algortihm to classify each observation to a pre-defined number of groups. To do this it looks for statistical similarities between the observations. For example, grouping animals that are similar given information about their size and speed."]},{"cell_type":"markdown","metadata":{"id":"xTAzbf_svlPp","colab_type":"text"},"source":["## Question 3\n","\n","*What do we mean when we say that a machine learning system is overfitting? (5%)*\n","\n","Overfitting occurs when a machine learning system produces an algorithm that too specifically fits itself to the training data provided. This will often result in high accuracy on that one dataset, but low accuracy when applied to new, unseen data. A system that is overfitting will try to over complicate the model and will not take into account the element of randomness (known as white noise) that occurs in each observation."]},{"cell_type":"markdown","metadata":{"id":"C1c5lsWpvlPp","colab_type":"text"},"source":["# Practical (85%)"]},{"cell_type":"markdown","metadata":{"id":"ZVis3hXtvlPq","colab_type":"text"},"source":["## Question 1\n","\n","*Your algorithm gets the following results in a classification experiment. Please compute the precision, recall, f-measure and accuracy manually (without the help of your computer/Python, please provide all steps and formulas). Include the process to get to the final result. (20%)*\n","\n","Id | Prediction | Gold\n","- | - | -\n","1 | True | True\n","2 | True | True\n","3 | False | True\n","4 | True | True\n","5 | False | True\n","6 | False | True\n","7 | True | True\n","8 | True | True\n","9 | True | True\n","10 | False | False\n","11 | False | False\n","12 | False | False\n","13 | True | False\n","14 | False | False\n","15 | False | False\n","16 | False | False\n","17 | False | False\n","18 | True | False\n","19 | True | False\n","20 | False | False"]},{"cell_type":"markdown","metadata":{"id":"NRHAfGayvlPr","colab_type":"text"},"source":["To compute the evaluation measures, we first need to construct the confusion matrix. This can be done by manually counting the combinations of True and False between the prediction and the gold standard i.e. True/True, True/False, False/True, False/False.\n","\n","Confusion Matrix | Predicted: True | Predicted: False | Total\n","- | - | - | -\n","Actual: True | 6 (TP) | 3 (FP) | 9\n","Actual: False | 3 (FN) | 8 (TN) | 11\n","Total | 9 | 11 | 20\n","\n","Where TP is the number of True Positives, FP is False Positives, FN is False Negatives, and TN is True Negatives. Now we can compute the following measures;"]},{"cell_type":"markdown","metadata":{"id":"WqBAV_ufvlPs","colab_type":"text"},"source":["### Accuracy\n","\n","Defined as the proportion of individuals correctly classified.\n","\n","$Accuracy = \\dfrac{TP + TN}{TP + TN + FP + FN}$\n","\n","$Accuracy = \\dfrac{6 + 8}{6 + 8 + 3 + 3}$\n","\n","$Accuracy = \\dfrac{14}{20}$\n","\n","$Accuracy = 0.7$"]},{"cell_type":"markdown","metadata":{"id":"ANCgPNEpvlPs","colab_type":"text"},"source":["### Precision\n","\n","Defined as the proportion of positive predictions that are correct.\n","\n","$Precision = \\dfrac{TP}{TP + FP}$\n","\n","$Precision = \\dfrac{6}{6 + 3}$\n","\n","$Precision = \\dfrac{2}{3}$\n","\n","$Precision = 0.667$ $(3dp)$"]},{"cell_type":"markdown","metadata":{"id":"RqeTZOFFvlPt","colab_type":"text"},"source":["### Recall\n","\n","Defined as the proportion of positive cases that were predicted as positive.\n","\n","$Recall = \\dfrac{TP}{TP + FN}$\n","\n","$Recall = \\dfrac{6}{6 + 3}$\n","\n","$Recall = \\dfrac{2}{3}$\n","\n","$Recall = 0.667$ $(3dp)$"]},{"cell_type":"markdown","metadata":{"id":"DeS5NcFSvlPu","colab_type":"text"},"source":["### F-Measure\n","\n","Defined as the harmonic mean of precision and recall. It's used to evaluate the performance of an algorithm in which you aren't neccesarily trying to improve only the precision or only the recall.\n","\n","$F_1 = 2 \\times \\dfrac{precision \\times recall}{precision + recall}$\n","\n","$F_1 = 2 \\times \\dfrac{\\dfrac{2}{3} \\times \\dfrac{2}{3}}{\\dfrac{2}{3} + \\dfrac{2}{3}}$\n","\n","$F_1 = 2/3$\n","\n","$F_1 = 0.667$ $(3dp)$"]},{"cell_type":"markdown","metadata":{"id":"RnQXI6UgvlPu","colab_type":"text"},"source":["## Question 2\n","\n","*You are given a dataset (named Wine dataset) with different measured properties of different wines (dataset available in Learning Central). Your goal is to develop a machine learning model to predict the quality of an unseen wine given these properties. Train two machine learning regression models and check their performance. Write, for each of the models, the main Python instructions to train and predict the labels (one line each, no need to include any data preprocessing) and the performance in the test set in terms of Root Mean Squared Error (RMSE) (30%)* "]},{"cell_type":"markdown","metadata":{"id":"Pl45JDZOvlPv","colab_type":"text"},"source":["### Load necessary modules "]},{"cell_type":"code","metadata":{"id":"7VrEarARvlPw","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","from math import sqrt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"El-QAHyHvlPy","colab_type":"text"},"source":["### Read in training and test data"]},{"cell_type":"code","metadata":{"id":"NKJPe_2ivlPz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"f8ae2319-c556-40c0-f0f4-47b408baa7a8","executionInfo":{"status":"error","timestamp":1578161331124,"user_tz":0,"elapsed":1006,"user":{"displayName":"shuai han","photoUrl":"","userId":"17730290437654356068"}}},"source":["wine_train = pd.read_csv(\"Data/Wine/wine_train.csv\", sep=';')\n","wine_test = pd.read_csv(\"Data/Wine/wine_test.csv\", sep=';')"],"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-a33c2ef17b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwine_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/Wine/wine_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwine_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/Wine/wine_test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Data/Wine/wine_train.csv' does not exist: b'Data/Wine/wine_train.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"mT9HB8G7vlP1","colab_type":"text"},"source":["### Model 1\n","\n","Least squares linear regression will be used to predict wine quality for the first model. It will use only one predictor variable; residual sugar. To begin, split the wine dataset by input variable (residual sugar) and output variable (wine quality). Repeat this with the test set. "]},{"cell_type":"code","metadata":{"id":"XyKCUIVDvlP1","colab_type":"code","colab":{}},"source":["wine_train_x_m1 = wine_train[['residual sugar']]\n","wine_train_y = wine_train['quality']\n","wine_test_x_m1 = wine_test[['residual sugar']]\n","wine_test_y = wine_test['quality']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kfOkhZIxvlP4","colab_type":"text"},"source":["Start by fitting a linear regression model to the training data. This will perform least squares regression which attempts to fit a model that minimises the sums of squares between the mapping function and the data. As the model is given the output variable (quality) this is a supervised machine learning model. Once trained, the `.predict` method can be called to predict wine quality of a new batch of wines, given their residual sugar levels. After this, root mean square error provides a measure of performance of the model and can be used to compare accuracy with other models."]},{"cell_type":"code","metadata":{"id":"A3iaUgq9vlP4","colab_type":"code","colab":{}},"source":["lin_reg_m1 = LinearRegression()\n","lin_reg_m1.fit(wine_train_x_m1, wine_train_y)\n","predictions_m1 = lin_reg_m1.predict(wine_test_x_m1)\n","rmse_m1 = sqrt(mean_squared_error(wine_test_y, predictions_m1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pcWEIeMJvlP6","colab_type":"text"},"source":["### Model 2\n","\n","Least squares linear regression will be used to predict wine quality for the second model. It will use two predictor variables; alcohol and volatile acidity."]},{"cell_type":"code","metadata":{"id":"MMr3vVU_vlP7","colab_type":"code","colab":{}},"source":["wine_train_x_m2 = wine_train[['alcohol', 'volatile acidity']]\n","wine_test_x_m2 = wine_test[['alcohol', 'volatile acidity']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bs_mzHCrvlP9","colab_type":"code","colab":{}},"source":["lin_reg_m2 = LinearRegression()\n","lin_reg_m2.fit(wine_train_x_m2, wine_train_y)\n","predictions_m2 = lin_reg_m2.predict(wine_test_x_m2)\n","rmse_m2 = sqrt(mean_squared_error(wine_test_y, predictions_m2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1x-yCh5gvlP_","colab_type":"text"},"source":["### Compare models"]},{"cell_type":"code","metadata":{"id":"Fv8deDccvlQA","colab_type":"code","colab":{},"outputId":"75f0329e-51b1-492b-90de-5488d129d63a"},"source":["print('Root Mean Square Error for Model 1 \\n----------')\n","print(round(rmse_m1, 3))\n","print('\\n Root Mean Square Error for Model 2 \\n----------')\n","print(round(rmse_m2, 3))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Root Mean Square Error for Model 1 \n","----------\n","0.886\n","\n"," Root Mean Square Error for Model 2 \n","----------\n","0.788\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e4TlVp_YvlQC","colab_type":"text"},"source":["### Conclusion\n","\n","Comparing the root mean square error (RMSE) of both models indicates that model 2 outperformed model 1. The root mean square error measures the difference between the predictions and the 'true' results. Therefore a smaller RMSE indicates a more accurate model. That is, by including both alcohol and volatile acidity in the model, better predictions of wine quality are achieved than if only residual sugar is used."]},{"cell_type":"markdown","metadata":{"id":"IjDIM92UvlQD","colab_type":"text"},"source":["## Question 3 (Code)"]},{"cell_type":"markdown","metadata":{"id":"DkMwK96PvlQD","colab_type":"text"},"source":["*Train an SVM binary classifier using the Hateval dataset (available in Learning\n","Central). The task consists of predicting whether a tweet represents hate speech\n","or not. You can preprocess and choose the features freely. Evaluate the\n","performance of your classifier in terms of accuracy using 10-fold cross-validation.\n","Write a table with the results of the classifier (accuracy, precision, recall and\n","F-measure) in each of the folds and write a small summary (up to 500 words) of\n","how you preprocessed the data, chose the feature/s, and trained and evaluated\n","your model (35%)*"]},{"cell_type":"markdown","metadata":{"id":"4gsoHEkfvlQE","colab_type":"text"},"source":["### Load modules"]},{"cell_type":"code","metadata":{"id":"n147fmyKvlQE","colab_type":"code","colab":{}},"source":["import nltk\n","import sklearn\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score,recall_score,f1_score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-l_BmmGHvlQG","colab_type":"text"},"source":["### Inspect data"]},{"cell_type":"code","metadata":{"id":"K34tiz-1vlQH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"outputId":"c48bc05e-35ce-4a96-ca05-a5cb3476ab79","executionInfo":{"status":"error","timestamp":1578161441981,"user_tz":0,"elapsed":1073,"user":{"displayName":"shuai han","photoUrl":"","userId":"17730290437654356068"}}},"source":["hateval = pd.read_csv(\"C:\\Users\\shylo\\Desktop\\Hateval\\hateval.tsv\", delimiter='\\t')\n","hateval.head()"],"execution_count":5,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-3f11494c7a43>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    hateval = pd.read_csv(\"C:\\Users\\shylo\\Desktop\\Hateval\\hateval.tsv\", delimiter='\\t')\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"]}]},{"cell_type":"code","metadata":{"id":"KC6jB0BovlQJ","colab_type":"code","colab":{},"outputId":"cb2b3132-fe75-4e03-889c-07d3810731ab"},"source":["print(\"Total no of tweets \\n------\")\n","print(hateval.label.count())\n","print(\"\\nOf which considered hate speech \\n------\")\n","print(hateval.label.sum())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total no of tweets \n","------\n","9000\n","\n","Of which considered hate speech \n","------\n","3783\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4Ki30E-kvlQL","colab_type":"text"},"source":["### Split data into training and test set"]},{"cell_type":"code","metadata":{"id":"rq23VAHfvlQM","colab_type":"code","colab":{}},"source":["hateval_train, hateval_test = train_test_split(hateval, train_size=0.8, random_state = 42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQV4Gcv3vlQO","colab_type":"code","colab":{},"outputId":"c2e00f41-a59d-43a1-e331-3c5bfd56733f"},"source":["print(hateval_train.shape)\n","print(hateval_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(7200, 3)\n","(1800, 3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GtCcVTzMvlQQ","colab_type":"text"},"source":["### Check proportions are similar between training and test"]},{"cell_type":"code","metadata":{"id":"OftU6kgZvlQR","colab_type":"code","colab":{},"outputId":"19a4af94-4998-472d-fbfc-d2591f45e03c"},"source":["hateval_train['label'].value_counts() / len(hateval_train['label'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    0.577361\n","1    0.422639\n","Name: label, dtype: float64"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"b8FadVZcvlQU","colab_type":"code","colab":{},"outputId":"732b5ab8-b9b2-429d-94a0-7f711c28ea91"},"source":["hateval_test['label'].value_counts() / len(hateval_test['label'])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    0.588889\n","1    0.411111\n","Name: label, dtype: float64"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"X9m4CbtkvlQY","colab_type":"text"},"source":["### Create functions"]},{"cell_type":"markdown","metadata":{"id":"teCO8X6gvlQY","colab_type":"text"},"source":["#### Create a list of unique tokens that have been lemmatized and made lower case"]},{"cell_type":"code","metadata":{"id":"2q5ZSfNavlQZ","colab_type":"code","colab":{}},"source":["lemmatizer = nltk.stem.WordNetLemmatizer()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rymij-qKvlQc","colab_type":"code","colab":{}},"source":["def get_tokens_vocab(df):\n","    list_tokens=[]\n","    for index, row in df.iterrows():\n","      sentence_split=nltk.tokenize.sent_tokenize(row['text'])\n","      for sentence in sentence_split:\n","        tokens = nltk.tokenize.word_tokenize(sentence)\n","        for token in tokens:\n","          list_tokens.append(lemmatizer.lemmatize(token).lower())\n","    return(list_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7nMo_GldvlQe","colab_type":"code","colab":{}},"source":["def get_tokens(string):\n","    sentence_split=nltk.tokenize.sent_tokenize(string)\n","    list_tokens=[]\n","    for sentence in sentence_split:\n","      list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n","      for token in list_tokens_sentence:\n","        list_tokens.append(lemmatizer.lemmatize(token).lower())\n","    return list_tokens"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cpk-KITBvlQf","colab_type":"text"},"source":["#### Create set of stopwords that will be removed later"]},{"cell_type":"code","metadata":{"id":"0lPvPvbmvlQg","colab_type":"code","colab":{}},"source":["# take set of stopwords from nltk\n","stopwords=set(nltk.corpus.stopwords.words('english'))\n","# manually add more punctuation\n","stopwords.add(\".\")\n","stopwords.add(\",\")\n","stopwords.add(\"--\")\n","stopwords.add(\"``\")\n","stopwords.add(\"#\")\n","stopwords.add(\"@\")\n","stopwords.add(\":\")\n","stopwords.add(\"'s\")\n","stopwords.add(\"â€™\")\n","stopwords.add(\"...\")\n","stopwords.add(\"n't\")\n","stopwords.add(\"'\")\n","stopwords.add(\"-\")\n","stopwords.add(\";\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7lErIFO4vlQi","colab_type":"text"},"source":["#### Sort tokens in dictionary to include the top n most used words"]},{"cell_type":"code","metadata":{"id":"LwsuTx9tvlQi","colab_type":"code","colab":{}},"source":["def sort_tokens(tokens, n):\n","    dict_word_freq={}\n","    for token in tokens:\n","        if token in stopwords: continue\n","        elif token not in dict_word_freq: dict_word_freq[token]=1\n","        else: dict_word_freq[token]+=1\n","    sorted_tokens = sorted(dict_word_freq.items(), key=lambda x: x[1], reverse=True)[:n]\n","    return(sorted_tokens)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"av526aPSvlQk","colab_type":"text"},"source":["#### Return the vocabulary to be used as features in SVM"]},{"cell_type":"code","metadata":{"id":"9YBjUpBGvlQl","colab_type":"code","colab":{}},"source":["def get_vocabulary(sorted_tokens):\n","    vocabulary=[]\n","    for word,frequency in sorted_tokens:\n","        vocabulary.append(word)\n","    return(np.asarray(vocabulary))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ie8yl_8YvlQm","colab_type":"text"},"source":["#### Return features as an array of counts"]},{"cell_type":"code","metadata":{"id":"bN3Xj2ffvlQn","colab_type":"code","colab":{}},"source":["def get_features(df, vocabulary):\n","    features_array=[]\n","    for index, row in df.iterrows():\n","        tokens=get_tokens(row['text'])\n","        features=np.zeros(len(vocabulary))\n","        for i, word in enumerate(vocabulary):\n","            if word in tokens:\n","                features[i]=tokens.count(word)\n","        features_array.append(features)\n","    return np.asarray(features_array)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mpzhp6QBvlQp","colab_type":"text"},"source":["#### Train SVM classifier"]},{"cell_type":"code","metadata":{"id":"On946nhovlQq","colab_type":"code","colab":{}},"source":["def train_svm_classifier(train, vocabulary):\n","    x_train = get_features(train, vocabulary)\n","    y_train = np.asarray(train['label'])\n","    svm_clf = sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n","    svm_clf.fit(x_train,y_train)\n","    return(svm_clf)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hsHurURCvlQw","colab_type":"text"},"source":["### Run `hateval` data through functions for 10 fold cross-valdation"]},{"cell_type":"code","metadata":{"id":"ISksw1GZvlQw","colab_type":"code","colab":{},"outputId":"7a9d464d-556d-4778-ceb6-21bf35772a38"},"source":["# need to find way to loop through each 10 folds and build model\n","kf = KFold(n_splits=10, random_state=42, shuffle=True)\n","accuracy_total = precision_total = recall_total = f_measure_total = 0.0\n","fold = 0\n","for train_index, test_index in kf.split(hateval):\n","    cv_train = hateval.iloc[train_index]\n","    cv_test = hateval.iloc[test_index]\n","    \n","    cv_tokens = get_tokens_vocab(cv_train)\n","    cv_tokens_sorted = sort_tokens(cv_tokens, 100)\n","    cv_vocab = get_vocabulary(cv_tokens_sorted)\n","    svm_clf = train_svm_classifier(cv_train, cv_vocab)\n","    \n","    cv_test_x = get_features(cv_test, cv_vocab)\n","    cv_test_y = np.asarray(cv_test['label'])\n","    y_test_predictions = svm_clf.predict(cv_test_x)\n","    \n","    accuracy_fold=accuracy_score(cv_test_y, y_test_predictions)\n","    precision_fold=precision_score(cv_test_y, y_test_predictions)\n","    recall_fold=recall_score(cv_test_y, y_test_predictions)\n","    f_measure_fold=f1_score(cv_test_y, y_test_predictions)\n","    accuracy_total+=accuracy_fold\n","    precision_total+=precision_fold\n","    recall_total+=recall_fold\n","    f_measure_total+=f_measure_fold\n","    \n","    fold+=1\n","    print(\"\\nFold \"+str(fold)+\"\\n-----\")\n","    print(\"Accuracy: \"+str(round(accuracy_fold, 3)))\n","    print(\"Precision: \"+str(round(precision_fold, 3)))\n","    print(\"Recall: \"+str(round(recall_fold, 3)))\n","    print(\"F-measure: \"+str(round(f_measure_fold, 3)))\n","    \n","average_accuracy=accuracy_total/10\n","average_precision=precision_total/10\n","average_recall=recall_total/10\n","average_f_measure=f_measure_total/10"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Fold 1\n","-----\n","Accuracy: 0.76\n","Precision: 0.786\n","Recall: 0.58\n","F-measure: 0.668\n","\n","Fold 2\n","-----\n","Accuracy: 0.757\n","Precision: 0.779\n","Recall: 0.56\n","F-measure: 0.652\n","\n","Fold 3\n","-----\n","Accuracy: 0.728\n","Precision: 0.741\n","Recall: 0.508\n","F-measure: 0.603\n","\n","Fold 4\n","-----\n","Accuracy: 0.719\n","Precision: 0.725\n","Recall: 0.527\n","F-measure: 0.61\n","\n","Fold 5\n","-----\n","Accuracy: 0.733\n","Precision: 0.814\n","Recall: 0.548\n","F-measure: 0.655\n","\n","Fold 6\n","-----\n","Accuracy: 0.749\n","Precision: 0.812\n","Recall: 0.563\n","F-measure: 0.665\n","\n","Fold 7\n","-----\n","Accuracy: 0.722\n","Precision: 0.733\n","Recall: 0.516\n","F-measure: 0.606\n","\n","Fold 8\n","-----\n","Accuracy: 0.729\n","Precision: 0.738\n","Recall: 0.506\n","F-measure: 0.6\n","\n","Fold 9\n","-----\n","Accuracy: 0.748\n","Precision: 0.773\n","Recall: 0.556\n","F-measure: 0.647\n","\n","Fold 10\n","-----\n","Accuracy: 0.747\n","Precision: 0.758\n","Recall: 0.586\n","F-measure: 0.661\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZFDE--qUvlQy","colab_type":"code","colab":{},"outputId":"6357fcac-3957-43b0-df93-3696d4162bcf"},"source":["print(\"Average Accuracy: \"+str(round(average_accuracy, 3)))\n","print(\"Average Precision: \"+str(round(average_precision, 3)))\n","print(\"Average Recall: \"+str(round(average_recall, 3)))\n","print(\"Average F-measure: \"+str(round(average_f_measure, 3)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Average Accuracy: 0.739\n","Average Precision: 0.766\n","Average Recall: 0.545\n","Average F-measure: 0.637\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lHGOQyJbvlQ0","colab_type":"text"},"source":["## Question 3 (Report)"]},{"cell_type":"markdown","metadata":{"id":"AfFKjwPRvlQ1","colab_type":"text"},"source":["To begin I calculated the proportion of tweets considered hate speech. This gives a good benchmark for our model as it should perform at least as better as if we always guessed the outcome that was more common. Of the 9,000 tweets, 3,783 were flagged as hate speech, this translates to 42%. Therefore if we always guessed that a tweet was not hateful, we would be correct 58% of the time.\n","\n","To preprocess the data I split each tweet into tokens. This means splitting it into it's component words, symbols, and punctuation. To ensure similar words weren't being double counted I also lemmatized each token. Without lemmatization \"shoe\" and \"shoes\" are counted as different words for example. I then removed stopwords from this list of unique tokens, common stopwords were removed using the `nltk` module, I also added some common punctuation to the list of stopwords such as full stops and commas. I then sorted the list of tokens from most frequent to least frequent and took the top 100 to be my attributes to use in the SVM classifier. I believed this provided enough information for the classifier but it's a parameter that could easily be tweaked to discover the most appropriate value. \n","\n","I then counted the number of occurences of each of these attributes in every different tweet. This produced a numeric list of length 100 for each tweet in the data. Now it's in a form the a SVM classifier can use. Instead of splitting the data into one training and one test set I used 10-fold cross validation. This is a good way to maximise the amount of data your machine learning model can use but it can also be computationally expensive if your data is large. For each fold of the cross-validation the  data was split into a training and test set and the attributes were calculated for that fold only. I then fit a SVM linear classifier and recorded accuracy, precision, recall, and F-measure for each fold. \n","\n","The table below shows the results of this process:\n","\n","Fold | Accuracy | Precision | Recall | F-measure\n","- | - | - | - | -\n","1 | 0.760 | 0.786 | 0.580 | 0.668\n","2 | 0.757 | 0.779 | 0.560 | 0.652\n","3 | 0.728 | 0.741 | 0.508 | 0.603\n","4 | 0.719 | 0.725 | 0.527 | 0.610\n","5 | 0.733 | 0.814 | 0.548 | 0.655\n","6 | 0.749 | 0.812 | 0.563 | 0.665\n","7 | 0.722 | 0.733 | 0.516 | 0.606\n","8 | 0.729 | 0.738 | 0.506 | 0.600\n","9 | 0.748 | 0.773 | 0.556 | 0.647\n","10 | 0.747 | 0.758 | 0.586 | 0.661\n","Total | 0.739 | 0.766 | 0.545 | 0.637\n","\n","We can see the accuracy of the model is around 74%. This seems significantly more accuracte than if we guessed every tweet was not hate speech (58%). Interestingly, recall is lower than all other measures, averaging at 55% across the 10 folds. Recall is the proportion of hate tweets that were correctly predicted as hate speech. That is, of the tweets that were considered hate speech, our model correctly predicted 55% of them. Meaning the other 45% were predicted not to be hateful. This is where the context of the problem is key, as this will determine which measure we are trying to maximise. If we are only interested in flagging as many hateful tweets as possible then we would look to build a model that maximises recall. However, we may not want to incorrectly flag tweets as hate speech as this may anger twitter users, in which case we'd want to maximise precision. "]}]}